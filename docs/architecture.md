为 Oasis AI 设计的整体架构蓝图：

---

### 一、 系统四层架构设计

我们将系统自上而下分为四层。每一层只与相邻的层通信，确保环境差异（Mac vs Kali）只影响最外层。

#### 1. 接入层 (Interface Layer)

* **职责：** 接收玩家输入，返回 AI 生成的响应。
* **组件：**
* **RESTful / WebSocket API：** 使用 Go 的 `net/http` 或高性能框架（如 Gin）暴露标准接口。
* **前端 UI (可选)：** 如果你们自带界面，可以打包为静态文件，由 Go 服务侧直接提供路由代理；或者直接兼容 SillyTavern 的 API 格式，让酒馆作为前端。



#### 2. 核心编排层 (Orchestrator Layer) - **[系统的绝对大脑，由 Go 语言实现]**

* **职责：** 控制“多模型流水线”，管理并发，处理跨平台路径差异。这是你主要编写代码的地方。
* **组件：**
* **Pipeline Manager (流水线管理器)：** 严格执行 `预处理 (模型A) -> 生成 (模型B) -> 后处理 (模型A)` 的调度逻辑。可以利用 Goroutines 实现异步非阻塞的状态监控。
* **Config Loader (环境探测器)：** 启动时动态读取 `.env` 或 `config.yaml`。如果是 Docker 环境，读取容器路径；如果是 Kali 原生环境，读取本地路径。
* **Context Router (上下文路由)：** 决定哪些历史对话需要被压缩，哪些可以直接扔给模型。



#### 3. 记忆与持久化层 (Memory & Storage Layer)

* **职责：** 解决“串记忆”问题，安全且离线地存储玩家数据。
* **组件：**
* **关系型元数据：** 使用 **SQLite**（Go 中直接引入驱动，无需额外部署数据库服务端）。用于存储玩家信息、NPC 设定、对话的轮次索引。
* **向量记忆库：** 考虑到极致离线，避免引入沉重的独立服务端，可以采用轻量级的嵌入式向量库（如 Go 语言原生的向量搜索实现，或轻量化的 ChromaDB/Qdrant 离线模式），存储历史对话的 Embedding 切片。



#### 4. 模型推理层 (Inference Engine Layer)

* **职责：** 纯粹的计算资源，加载 GGUF 权重并执行推理。
* **组件：**
* **本地推理后端：** 统一使用 **Ollama** 或 **llama.cpp** 的 Server 模式。
* **模型池：**
* **模型 A (Router/Logic)：** 例如轻量级的 Qwen-2.5-1.5B，极低显存占用，专职做摘要和格式化。
* **模型 B (Roleplay)：** 玩家自行下载的较大参数角色扮演模型。





---

### 二、 核心流水线数据流向 (Data Flow)

当玩家发送一条消息（例如：“你还记得我们在落日森林遇到的那个剑客吗？”）时，系统内部的流转如下：

1. **玩家输入** 到达 `接入层`。
2. **触发检索：** `编排层` 将输入转化为向量，去 `记忆层` 的向量库中检索关于“落日森林”和“剑客”的旧记忆记录。
3. **模型 A (预处理)：** `编排层` 将“玩家输入 + 检索到的旧记忆”发给模型 A，要求其生成一段精简的**背景提要**（System Prompt Extension）。
4. **模型 B (生成)：** `编排层` 将组合好的“完美上下文”发给模型 B。模型 B 沉浸在角色中，生成符合设定的情感回复。
5. **模型 A (后处理 / 可选)：** `编排层` 让模型 A 快速检查一遍模型 B 的回复是否有严重的逻辑漏洞（比如 NPC 透露了不该说的话）。
6. **输出返回：** 最终结果推给 `接入层`，展示给玩家，同时这段新对话被异步写入 SQLite 和向量库。

---

### 三、 跨平台的工程目录结构建议

为了让你的 Docker 开发流和你朋友的 Kali 原生开发流不打架，推荐采用标准且严谨的 Go 工程布局：

```text
Oasis-AI/
├── cmd/
│   └── oasis/
│       └── main.go           # 唯一入口，负责初始化日志、读取配置、启动 HTTP 服务
├── internal/                 # 核心业务逻辑（不对外暴露）
│   ├── config/               # 跨平台配置解析 (处理路径差异)
│   ├── orchestrator/         # 多模型 A-B-A 流水线调度
│   ├── memory/               # SQLite 和 向量库的操作封装
│   └── llm/                  # 与 Ollama API 交互的客户端封装
├── data/                     # 数据目录 (SQLite 数据库文件，向量索引) -> 已加入 .gitignore
├── deploy/                   # 部署相关
│   ├── docker/               # 你的 Dockerfile 和 docker-compose.yml
│   └── appimage/             # (alix)未来用于打包 AppImage 的构建脚本
├── Makefile                  # 核心协作工具！统一定义 run, build, test 命令
├── go.mod
└── README.md

```

---

### 四、 解决协作冲突的“架构级”约定

架构设计中必须明确以下原则：

1. **绝对路径禁令：** 代码中绝不出现 `/Users/xxx` 或 `/home/xxx`。所有涉及文件读写（如数据库位置、日志位置）全部通过 `internal/config` 模块按相对路径或环境变量动态生成。
2. **网络层解耦：** Go 代码只通过 `http://127.0.0.1:11434` 与 Ollama 通信。在Sisyphe的 Docker 中，配置网络让其指向宿主机；在Alix的 Kali 中，这就直接指向alix本地的 Ollama 服务。代码一行都不用改。

按照这个架构，Oasis AI 会非常健壮，既能满足现代服务端的部署标准，也能完美降级为纯粹的单机离线应用。
